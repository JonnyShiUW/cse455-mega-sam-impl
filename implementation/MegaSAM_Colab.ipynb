{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MegaSAM Pipeline - Google Colab\n",
    "\n",
    "This notebook runs the MegaSAM pipeline for camera tracking and depth estimation.\n",
    "\n",
    "**Requirements:** GPU runtime (Runtime > Change runtime type > T4 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone Repository and Initialize Submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository\n!git clone https://github.com/JonnyShiUW/cse455-mega-sam-impl.git\n%cd cse455-mega-sam-impl\n\n# Initialize submodules\n!git submodule update --init --recursive"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use Colab's pre-installed PyTorch (compatible with the environment)\n# Just install the additional dependencies\n!pip install opencv-python-headless tqdm imageio einops scipy matplotlib \n!pip install timm ninja numpy==1.26.3 huggingface-hub kornia\n!pip install torch-scatter -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__.split('+')[0])\")+cu$(python -c \"import torch; print(torch.version.cuda.replace('.',''))\").html"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install xformers (compatible with Colab's PyTorch version)\n!pip install xformers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install UniDepth\n",
    "!pip install unidepth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compile DROID-SLAM Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd DROID-SLAM\n",
    "!python setup.py install\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directories\n",
    "!mkdir -p mega-sam/Depth-Anything/checkpoints\n",
    "\n",
    "# Download DepthAnything checkpoint (~1.2GB)\n",
    "!wget -O mega-sam/Depth-Anything/checkpoints/depth_anything_vitl14.pth \\\n",
    "    \"https://huggingface.co/spaces/LiheYoung/Depth-Anything/resolve/main/checkpoints/depth_anything_vitl14.pth\"\n",
    "\n",
    "print(\"DepthAnything checkpoint downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download RAFT checkpoint (~78MB)\n",
    "!wget -O mega-sam/cvd_opt/raft-things.pth \\\n",
    "    \"https://www.dropbox.com/s/4j4z58wuv8o0mfz/raft-things.pth?dl=1\"\n",
    "\n",
    "print(\"RAFT checkpoint downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify checkpoints\n",
    "!ls -lh mega-sam/Depth-Anything/checkpoints/\n",
    "!ls -lh mega-sam/cvd_opt/raft-things.pth\n",
    "!ls -lh mega-sam/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload Your Input Frames\n",
    "\n",
    "Upload your `test_video` folder of JPEG frames, or use the sample frames if included in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if test_video frames exist\n",
    "!ls test_video/ | head -10\n",
    "!ls test_video/*.jpg 2>/dev/null | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to upload frames from Google Drive:\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !cp -r /content/drive/MyDrive/your_frames_folder ./test_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run MegaSAM Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify setup before running\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline!\n",
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Visualizations for Presentation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check outputs\n",
    "!ls -lh outputs_cvd/\n",
    "!ls -lh reconstructions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect final output\n",
    "import numpy as np\n",
    "\n",
    "data = np.load(\"outputs_cvd/marching_sgd_cvd_hr.npz\")\n",
    "print(\"Output contents:\")\n",
    "for key in data.files:\n",
    "    print(f\"  {key}: shape={data[key].shape}, dtype={data[key].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample depth map\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RGB frame\n",
    "axes[0].imshow(data['images'][0])\n",
    "axes[0].set_title('Input Frame 0')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Depth map\n",
    "axes[1].imshow(data['depths'][0], cmap='turbo')\n",
    "axes[1].set_title('Estimated Depth 0')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Grid View: Multiple Frames with Depth Maps",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Show grid of frames at different time points\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nn_samples = 6  # Number of frames to show\ntotal_frames = len(data['images'])\nindices = np.linspace(0, total_frames - 1, n_samples, dtype=int)\n\nfig, axes = plt.subplots(2, n_samples, figsize=(20, 7))\n\nfor i, idx in enumerate(indices):\n    # RGB frame\n    axes[0, i].imshow(data['images'][idx])\n    axes[0, i].set_title(f'Frame {idx}')\n    axes[0, i].axis('off')\n    \n    # Depth map\n    axes[1, i].imshow(data['depths'][idx], cmap='turbo')\n    axes[1, i].set_title(f'Depth {idx}')\n    axes[1, i].axis('off')\n\naxes[0, 0].set_ylabel('RGB', fontsize=14)\naxes[1, 0].set_ylabel('Depth', fontsize=14)\n\nplt.suptitle('MegaSAM: RGB Frames and Estimated Depth Maps', fontsize=16)\nplt.tight_layout()\nplt.savefig('grid_visualization.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Saved: grid_visualization.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Camera Trajectory Visualization (3D)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Plot camera trajectory in 3D\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Extract camera positions from camera-to-world transforms\ncam_c2w = data['cam_c2w']  # Shape: (N, 4, 4)\npositions = cam_c2w[:, :3, 3]  # Extract translation component\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot trajectory\nax.plot(positions[:, 0], positions[:, 1], positions[:, 2], \n        'b-', linewidth=2, label='Camera Path')\n\n# Mark start and end\nax.scatter(*positions[0], color='green', s=100, label='Start', marker='o')\nax.scatter(*positions[-1], color='red', s=100, label='End', marker='s')\n\n# Color points by time\ncolors = plt.cm.viridis(np.linspace(0, 1, len(positions)))\nax.scatter(positions[:, 0], positions[:, 1], positions[:, 2], \n           c=colors, s=20, alpha=0.6)\n\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title('Estimated Camera Trajectory (MegaSAM)')\nax.legend()\n\nplt.tight_layout()\nplt.savefig('camera_trajectory.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Saved: camera_trajectory.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Animated GIF: Side-by-Side RGB and Depth",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create side-by-side animated GIF\nimport imageio\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\nfrom tqdm import tqdm\n\nframes_for_gif = []\ndepths = data['depths']\nimages = data['images']\n\n# Normalize depths for consistent colormap\ndepth_min, depth_max = depths.min(), depths.max()\n\nprint(\"Creating animation frames...\")\nfor i in tqdm(range(len(images))):\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # RGB\n    axes[0].imshow(images[i])\n    axes[0].set_title('Input RGB', fontsize=14)\n    axes[0].axis('off')\n    \n    # Depth with consistent colormap\n    depth_normalized = (depths[i] - depth_min) / (depth_max - depth_min)\n    depth_colored = cm.turbo(depth_normalized)[:, :, :3]\n    axes[1].imshow(depth_colored)\n    axes[1].set_title('Estimated Depth', fontsize=14)\n    axes[1].axis('off')\n    \n    plt.suptitle(f'MegaSAM Output - Frame {i}/{len(images)-1}', fontsize=16)\n    plt.tight_layout()\n    \n    # Convert figure to image\n    fig.canvas.draw()\n    frame = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n    frame = frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    frames_for_gif.append(frame)\n    plt.close(fig)\n\n# Save as GIF\nprint(\"Saving GIF...\")\nimageio.mimsave('megasam_output.gif', frames_for_gif, fps=10)\nprint(\"Saved: megasam_output.gif\")\n\n# Display in notebook\nfrom IPython.display import Image, display\ndisplay(Image(filename='megasam_output.gif'))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 8. Download Results"
  },
  {
   "cell_type": "code",
   "source": "# Download all visualization files for your presentation\nfrom google.colab import files\nimport shutil\n\n# Create a zip with all results\n!mkdir -p megasam_presentation\n!cp grid_visualization.png megasam_presentation/\n!cp camera_trajectory.png megasam_presentation/\n!cp megasam_output.gif megasam_presentation/\n!cp outputs_cvd/marching_sgd_cvd_hr.npz megasam_presentation/\n\nshutil.make_archive('megasam_presentation', 'zip', 'megasam_presentation')\n\nprint(\"Files included:\")\n!ls -lh megasam_presentation/\n\nprint(\"\\nDownloading zip file...\")\nfiles.download('megasam_presentation.zip')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or save to Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !cp -r outputs_cvd /content/drive/MyDrive/megasam_results/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}